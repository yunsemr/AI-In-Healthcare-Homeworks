{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUzFO3Za-AU4"
      },
      "source": [
        "### 1. Download the Data from Kaggle and Unzip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_g1YiJKHK5M8"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTnECrlJK7UP"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPS6a8XtLY1t",
        "outputId": "ac8718d8-e6bc-4d14-9377-79d3d4f6042f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading large-covid19-ct-slice-dataset.zip to /content\n",
            "100% 2.06G/2.06G [01:01<00:00, 37.8MB/s]\n",
            "100% 2.06G/2.06G [01:01<00:00, 35.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d 'maedemaftouni/large-covid19-ct-slice-dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5ghbm_EsCVK",
        "outputId": "15de7f58-5d3e-4b46-b470-e33e035189f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\") # Connect to google drive as we will save model weights here\n",
        "shutil.unpack_archive(\"/content/large-covid19-ct-slice-dataset.zip\", \"/tmp/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD8e2UClO0Wf"
      },
      "source": [
        "### 1. OR Unzip the data if you have uploaded it to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW5CWRBoPgIp",
        "outputId": "30ab5979-87f7-4020-f7f8-b332cd92c094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "# Change the code below if the path to the dataset is different for you.\n",
        "shutil.unpack_archive(\"/content/gdrive/MyDrive/archive.zip\", \"/tmp/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMKxclhvd9fz"
      },
      "source": [
        "### 2. Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6sKDOT9lO4P",
        "outputId": "1cf400da-66e2-4d4e-b9d0-f31fc44e1d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal\n",
            "Number of patient:  604\n",
            "Number of image:  6893\n",
            "\n",
            "Covid\n",
            "Number of patient:  464\n",
            "Number of image:  7593\n",
            "\n",
            "CAP\n",
            "Number of patient:  54\n",
            "Number of image:  2618\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "meta_normal = pd.read_csv(\"/tmp/meta_data_normal.csv\")\n",
        "meta_covid = pd.read_csv(\"/tmp/meta_data_covid.csv\", encoding='windows-1252')\n",
        "meta_cap = pd.read_csv(\"/tmp/meta_data_cap.csv\")\n",
        "\n",
        "# Define the variables below using meta dataframes\n",
        "\n",
        "normal_pt_nb = meta_normal['Patient ID'].nunique() # Number of patients in normal group\n",
        "covid_pt_nb = meta_covid['Patient ID'].nunique() # Number of patients in covid group\n",
        "cap_pt_nb = meta_cap['Patient ID'].nunique() # Number of patients in CAP group\n",
        "\n",
        "normal_img_nb = meta_normal['File name'].count() # Number of images in normal group\n",
        "covid_img_nb = meta_covid['File name'].count() # Number of images in covid group\n",
        "cap_img_nb = meta_cap['File name'].count() # Number of images in CAP group\n",
        "\n",
        "\n",
        "print(\"Normal\")\n",
        "print(\"Number of patient: \", normal_pt_nb)\n",
        "print(\"Number of image: \", normal_img_nb)\n",
        "\n",
        "print(\"\\nCovid\")\n",
        "print(\"Number of patient: \", covid_pt_nb)\n",
        "print(\"Number of image: \", covid_img_nb)\n",
        "\n",
        "print(\"\\nCAP\")\n",
        "print(\"Number of patient: \", cap_pt_nb)\n",
        "print(\"Number of image: \", cap_img_nb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBbpEW_6qVHJ",
        "outputId": "c8dbd0a6-9476-42c9-c2d5-d59696555ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slice-based val size: \n",
            "Normal:  0.21\n",
            "Covid:  0.18\n",
            "\n",
            "Slice-based test size: \n",
            "Normal:  0.51\n",
            "Covid:  0.5\n",
            "\n",
            "Slice-based train size: \n",
            "Normal:  0.29\n",
            "Covid:  0.32\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set seed to get the same result (I specifically chose this seed after a couple of tries so that we'll have approximately same split ratios on slice level as well)\n",
        "np.random.seed(58)\n",
        "val_split_size = .2\n",
        "test_split_size = .5\n",
        "\n",
        "normal_val_file_list, normal_test_file_list = [], []\n",
        "covid_val_file_list, covid_test_file_list = [], []\n",
        "##### START OF YOUR CODE #####\n",
        "patient_id_normal = meta_normal['Patient ID'].unique()\n",
        "patient_id_covid = meta_covid['Patient ID'].unique()\n",
        "\n",
        "#random shuffle\n",
        "np.random.shuffle(patient_id_normal)\n",
        "np.random.shuffle(patient_id_covid)\n",
        "\n",
        "#validation and test size\n",
        "normal_val_size = int(normal_pt_nb * val_split_size)\n",
        "normal_test_size = int(normal_pt_nb * test_split_size)\n",
        "covid_val_size = int(covid_pt_nb * val_split_size)\n",
        "covid_test_size = int(covid_pt_nb * test_split_size)\n",
        "\n",
        "\n",
        "pt_id_normal_val = patient_id_normal[0:normal_val_size]\n",
        "pt_id_normal_test = patient_id_normal[normal_val_size:normal_val_size + normal_test_size]\n",
        "pt_id_covid_val =  patient_id_covid[0:covid_val_size]\n",
        "pt_id_covid_test = patient_id_covid[covid_val_size:covid_val_size + covid_test_size]\n",
        "pt_id_covid_train = patient_id_covid[covid_val_size + covid_test_size:len(patient_id_covid)]\n",
        "pt_id_normal_train = patient_id_normal[normal_val_size + normal_test_size:len(patient_id_normal)]\n",
        "\n",
        "#empty lists for train split\n",
        "normal_train_file_list, covid_train_file_list = [], []\n",
        "\n",
        "#val, test split for normal\n",
        "for i in range(0, len(pt_id_normal_val)):\n",
        "  for j in range(0, len(meta_normal['Patient ID'])):\n",
        "    if pt_id_normal_val[i] == meta_normal['Patient ID'].iloc[j]:\n",
        "      normal_val_file_list.append(meta_normal['File name'].iloc[j])\n",
        "\n",
        "for i in range(0, len(pt_id_normal_test)):\n",
        "  for j in range(0, len(meta_normal['Patient ID'])):\n",
        "    if pt_id_normal_test[i] == meta_normal['Patient ID'].iloc[j]:\n",
        "      normal_test_file_list.append(meta_normal['File name'].iloc[j])\n",
        "#val, test split for covid\n",
        "for i in range(0, len(pt_id_covid_val)):\n",
        "  for j in range(0, len(meta_covid['Patient ID'])):\n",
        "    if pt_id_covid_val[i] == meta_covid['Patient ID'].iloc[j]:\n",
        "      covid_val_file_list.append(meta_covid['File name'].iloc[j])\n",
        "\n",
        "for i in range(0, len(pt_id_covid_test)):\n",
        "  for j in range(0, len(meta_covid['Patient ID'])):\n",
        "    if pt_id_covid_test[i] == meta_covid['Patient ID'].iloc[j]:\n",
        "      covid_test_file_list.append(meta_covid['File name'].iloc[j])\n",
        "#train split for normal, covid\n",
        "for i in range(0, len(pt_id_normal_train)):\n",
        "  for j in range(0, len(meta_normal['Patient ID'])):\n",
        "    if pt_id_normal_train[i] == meta_normal['Patient ID'].iloc[j]:\n",
        "      normal_train_file_list.append(meta_normal['File name'].iloc[j])      \n",
        "\n",
        "for i in range(0, len(pt_id_covid_train)):\n",
        "  for j in range(0, len(meta_covid['Patient ID'])):\n",
        "    if pt_id_covid_train[i] == meta_covid['Patient ID'].iloc[j]:\n",
        "      covid_train_file_list.append(meta_covid['File name'].iloc[j])\n",
        "##### END OF YOUR CODE #####\n",
        "\n",
        "print(\"Slice-based val size: \")\n",
        "print(\"Normal: \", round(len(normal_val_file_list)/normal_img_nb, 2))\n",
        "print(\"Covid: \", round(len(covid_val_file_list)/covid_img_nb, 2))\n",
        "\n",
        "print(\"\\nSlice-based test size: \")\n",
        "print(\"Normal: \", round(len(normal_test_file_list)/normal_img_nb, 2))\n",
        "print(\"Covid: \", round(len(covid_test_file_list)/covid_img_nb, 2))\n",
        "\n",
        "print(\"\\nSlice-based train size: \")\n",
        "print(\"Normal: \", round(len(normal_train_file_list)/normal_img_nb, 2))\n",
        "print(\"Covid: \", round(len(covid_train_file_list)/covid_img_nb, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdYip6S1qZh_",
        "outputId": "ae987bd7-3c4c-4afd-c8f5-c7f6d7f4e7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Percentage of train set: 0.30\n",
            "Percentage of Covid + slices in train set is: 0.55\n",
            "\n",
            "Percentage of val set: 0.19\n",
            "Percentage of Covid + slices in val set is: 0.49\n",
            "\n",
            "Percentage of test set: 0.51\n",
            "Percentage of Covid + slices in test set is: 0.52\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "##### START OF YOUR CODE #####\n",
        "import shutil\n",
        "\n",
        "#make the directories\n",
        "os.mkdir('/tmp/curated_data/data')\n",
        "for i in [\"train/\", \"test/\", \"val/\"]: \n",
        "  os.mkdir('/tmp/curated_data/data/'+i)\n",
        "  for j in [\"normal/\", \"covid/\"]:\n",
        "    os.mkdir(f'/tmp/curated_data/data/{i}'+j)\n",
        "\n",
        "#paths for moving operation\n",
        "src_normal_path = \"/tmp/curated_data/curated_data/1NonCOVID/\"\n",
        "src_covid_path = \"/tmp/curated_data/curated_data/2COVID/\"\n",
        "dst_normal_train_path = \"/tmp/curated_data/data/train/normal/\"\n",
        "dst_normal_test_path = \"/tmp/curated_data/data/test/normal/\"\n",
        "dst_normal_val_path = \"/tmp/curated_data/data/val/normal/\"\n",
        "dst_covid_train_path = \"/tmp/curated_data/data/train/covid/\"\n",
        "dst_covid_test_path = \"/tmp/curated_data/data/test/covid/\"\n",
        "dst_covid_val_path = \"/tmp/curated_data/data/val/covid/\"\n",
        "\n",
        "#moving normal train, test, val\n",
        "for i in normal_train_file_list:\n",
        "  for j in  os.listdir(src_normal_path):\n",
        "    if i == j:\n",
        "      shutil.move(src_normal_path+i, dst_normal_train_path)\n",
        "\n",
        "for i in normal_test_file_list:\n",
        "  for j in  os.listdir(src_normal_path):\n",
        "    if i == j:\n",
        "      shutil.move(src_normal_path+i, dst_normal_test_path)\n",
        "\n",
        "for i in normal_val_file_list:\n",
        "  for j in  os.listdir(src_normal_path):\n",
        "    if i == j:\n",
        "      shutil.move(src_normal_path+i, dst_normal_val_path)      \n",
        "\n",
        "#moving covid train, test, val\n",
        "for i in covid_train_file_list:\n",
        "  for j in  os.listdir(src_covid_path):\n",
        "    if i == j:\n",
        "      shutil.move(src_covid_path+i, dst_covid_train_path)\n",
        "\n",
        "for i in covid_test_file_list:\n",
        "  for j in  os.listdir(src_covid_path):\n",
        "    if i == j:\n",
        "      shutil.move(src_covid_path+i, dst_covid_test_path)\n",
        "\n",
        "for i in covid_val_file_list:\n",
        "  for j in  os.listdir(src_covid_path):\n",
        "    if i == j:\n",
        "      shutil.move(src_covid_path+i, dst_covid_val_path)      \n",
        "      \n",
        "    \n",
        "\n",
        "     \n",
        "\n",
        "\n",
        "##### END OF YOUR CODE #####\n",
        "\n",
        "data_counts = {x+y: len(os.listdir(\"/tmp/curated_data/data/\"+x+y)) for x in [\"train/\", \"val/\", \"test/\"] for y in [\"normal/\", \"covid/\"]}\n",
        "for i in [\"train\", \"val\", \"test\"]:\n",
        "  print(\"\\nPercentage of {} set: {:.2f}\" .format(i, (data_counts[i+\"/normal/\"]+data_counts[i+\"/covid/\"])/sum(data_counts.values())))\n",
        "  print(\"Percentage of Covid + slices in {} set is: {:.2f}\" .format(i, data_counts[i+\"/covid/\"]/(data_counts[i+\"/normal/\"]+data_counts[i+\"/covid/\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1LyhJcvl70h"
      },
      "source": [
        "### 3. Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtsYoaUfAN3i"
      },
      "outputs": [],
      "source": [
        "!pip install torchio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HT7wX3DxgxP-"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import Path\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchio as tio\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, image_size, data_folder, partition):\n",
        "    # Define attributes\n",
        "    ##### START OF YOUR CODE #####\n",
        "    self.image_size = image_size\n",
        "    self.data_folder = data_folder\n",
        "    self.partition = partition\n",
        "    \n",
        "    ##### END OF YOUR CODE #####\n",
        "    if data_folder.endswith(\"/\") == True:\n",
        "      self.paths = self.data_folder + self.partition # List of image paths\n",
        "    else:\n",
        "      self.paths = self.data_folder + \"/\" + self.partition\n",
        "    \n",
        "\n",
        "  def __len__(self):\n",
        "    ##### START OF YOUR CODE #####\n",
        "    tot_list = self.img_paths()\n",
        "    return len(tot_list)\n",
        "    ##### END OF YOUR CODE #####\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \n",
        "    ##### START OF YOUR CODE #####\n",
        "    image_size = self.image_size\n",
        "    partition = self.partition\n",
        "    data_folder = self.data_folder  \n",
        "    tot_list = self.img_paths()\n",
        "    path_img = tot_list[idx]\n",
        "\n",
        "    img = self.read_and_resize_img(path_img)\n",
        "\n",
        "    if partition == \"train\":\n",
        "      img = self.augmentation(img)\n",
        "\n",
        "    if \"covid\" in path_img:\n",
        "      label = np.ones(1)\n",
        "    else:\n",
        "      label = np.zeros(1)        \n",
        "    \n",
        "    return img, label\n",
        "    ##### END OF YOUR CODE #####\n",
        "\n",
        "  def img_paths(self):\n",
        "    ##### START OF YOUR CODE #####\n",
        "    normal_paths = [os.path.join(self.data_folder, self.partition, \"normal\", i) for i in os.listdir(os.path.join(self.data_folder, self.partition, \"normal\"))]\n",
        "    covid_paths = [os.path.join(self.data_folder, self.partition, \"covid\", i) for i in os.listdir(os.path.join(self.data_folder, self.partition, \"covid\"))]\n",
        "    paths = normal_paths + covid_paths\n",
        "    np.random.shuffle(paths)\n",
        "    \n",
        "    return paths\n",
        "    ##### END OF YOUR CODE #####\n",
        "\n",
        "  def read_and_resize_img(self, path):\n",
        "    ##### START OF YOUR CODE #####\n",
        "    img = Image.open(path)\n",
        "  \n",
        "    if img.size[1:] != (self.image_size, self.image_size):\n",
        "      transform = T.Compose([\n",
        "          T.Grayscale(),\n",
        "          T.Resize((self.image_size, self.image_size)),\n",
        "          T.ToTensor()\n",
        "      ])\n",
        "      img = transform(img)\n",
        "\n",
        "    else:\n",
        "      transform = T.Compose([\n",
        "         T.Grayscale(),\n",
        "         T.ToTensor()\n",
        "              ])\n",
        "      img = transform(img)\n",
        "\n",
        "    return img  \n",
        "    ##### END OF YOUR CODE #####\n",
        "\n",
        "  def augmentation(self, data):\n",
        "    ##### START OF YOUR CODE #####\n",
        "    transform = T.Compose([\n",
        "       T.RandomApply([\n",
        "          tio.transforms.RandomBiasField(p=0.2),\n",
        "          tio.transforms.RandomNoise(p=0.2),\n",
        "          tio.transforms.RandomGhosting(p=0.2),\n",
        "          tio.transforms.RandomSpike(p=0.2),\n",
        "          tio.transforms.RandomAffine(degrees=10, scales=0., translation=0., p=0.2)],\n",
        "          p=0.8)\n",
        "    ])\n",
        "\n",
        "    x = data.unsqueeze(-1) # x is a random variable to store data\n",
        "    x = transform(x)\n",
        "    data = x.squeeze(-1)\n",
        "    \n",
        "    return data\n",
        "    ##### END OF YOUR CODE #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP9qzi_p7Gm8"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# image_size = 256\n",
        "# data_dir = \"/tmp/curated_data/data\"\n",
        "# batch_size = 16\n",
        "\n",
        "# data_dict = {x: CustomDataset(image_size, data_dir, x) for x in [\"train\", \"val\", \"test\"]}\n",
        "# img_data, lab_data = next(iter(data_dict[\"train\"]))\n",
        "# print(img_data.shape, lab_data.shape)\n",
        "\n",
        "# dataloader_dict = {x: DataLoader(data_dict[x], batch_size) for x in [\"train\", \"val\", \"test\"]}\n",
        "# img, lab = next(iter(dataloader_dict[\"train\"]))\n",
        "# print(img.shape, lab.shape)\n",
        "\n",
        "\n",
        "# label_dict = {\n",
        "#     0: \"Normal\",\n",
        "#     1: \"Covid\"\n",
        "# }\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# for i in range(4):\n",
        "#   plt.subplot(2,2, i+1)\n",
        "#   plt.imshow(img[i, 0], cmap=\"gray\")\n",
        "#   plt.title(label_dict[int(lab[i].item())], fontsize=16)\n",
        "#   plt.axis(\"off\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEN_w6q3tbZM"
      },
      "source": [
        "### 4. ResNet-18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w4zlwn3H8ZUR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, s, act):\n",
        "      super(ConvBlock,self).__init__()\n",
        "      # Initialize layers\n",
        "      ##### START OF YOUR CODE #####\n",
        "      self.ch_in = ch_in\n",
        "      self.ch_out =ch_out\n",
        "      self.s = s\n",
        "      self.bn = nn.BatchNorm2d(ch_out)\n",
        "      self.act = act\n",
        "\n",
        "      if self.act.lower() == \"relu\":\n",
        "        self.act = nn.ReLU()\n",
        "      elif act.lower() == \"leaky_relu\":\n",
        "        self.act = nn.Leaky_ReLU()\n",
        "      elif act.lower() == \"gelu\":\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "      #convolutional layers of one block\n",
        "      if self.s == 1:\n",
        "        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size = (1,1), stride = s, padding = 0)\n",
        "        self.conv2 = nn.Conv2d(ch_in, ch_out, kernel_size = (3,3), stride = s, padding = 1)\n",
        "        self.conv3 = nn.Conv2d(ch_out, ch_out, kernel_size = (3,3), stride = s, padding = 1)    \n",
        "      else:\n",
        "        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size = (1,1), stride = s, padding = 0)\n",
        "        self.conv2 = nn.Conv2d(ch_in, ch_out, kernel_size = (3,3), stride = s, padding = 1)\n",
        "        self.conv3 = nn.Conv2d(ch_out, ch_out, kernel_size = (3,3), stride = s-1, padding = 1)\n",
        "      \n",
        "      ##### END OF YOUR CODE #####\n",
        "\n",
        "    def forward(self, X):\n",
        "      ##### START OF YOUR CODE #####      \n",
        "      x = self.conv1(X)\n",
        "      x = self.bn(x)\n",
        "      \n",
        "      y = self.conv2(X)\n",
        "      y = self.bn(y)\n",
        "      y = self.act(y)\n",
        "      y = self.conv3(y)\n",
        "      y = self.bn(y)\n",
        "\n",
        "      y += x # skip connection\n",
        "      X = self.act(y)\n",
        "      ##### END OF YOUR CODE #####\n",
        "      return X\n",
        "\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, act, drop_rate, image_size):\n",
        "      super(ResNet18, self).__init__()\n",
        "      # Initialize layers\n",
        "      ##### START OF YOUR CODE #####\n",
        "      self.drop_rate = drop_rate\n",
        "      self.drop_out = nn.Dropout2d(drop_rate)\n",
        "      self.max_pool = nn.MaxPool2d(kernel_size = (3,3), stride = (2,2), padding = (1,1))\n",
        "      self.image_size = image_size\n",
        "      kernel_size = self.image_size/2**5\n",
        "      self.avg_pool = nn.AvgPool2d(int(kernel_size), int(kernel_size))\n",
        "      self.fully_connected = nn.Linear(512,1)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "      self.bn = nn.BatchNorm2d(64)  \n",
        "      self.flatten = nn.Flatten()\n",
        "      self.act = act \n",
        "     \n",
        "      \n",
        "\n",
        "      if self.act.lower() == \"relu\":\n",
        "        self.act = nn.ReLU()\n",
        "      elif act.lower() == \"leaky_relu\":\n",
        "        self.act = nn.Leaky_ReLU()\n",
        "      elif act.lower() == \"gelu\":\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "      #convolutional blocks\n",
        "      self.conv1 = nn.Conv2d(1, 64, kernel_size = (7,7), stride = (2,2), padding = (3,3))\n",
        "      \n",
        "      self.conv2_x = nn.Sequential(ConvBlock(64, 64, 1, act),\n",
        "                                   ConvBlock(64, 64, 1, act))\n",
        "      \n",
        "      self.conv3_x = nn.Sequential(ConvBlock(64, 128, 2, act),\n",
        "                                   ConvBlock(128, 128, 1, act))\n",
        "      \n",
        "      self.conv4_x = nn.Sequential(ConvBlock(128, 256, 2, act),\n",
        "                                   ConvBlock(256, 256, 1, act))\n",
        "      \n",
        "      self.conv5_x = nn.Sequential(ConvBlock(256, 512, 2, act),\n",
        "                                   ConvBlock(512, 512, 1, act))\n",
        "          \n",
        "      ##### END OF YOUR CODE #####\n",
        "\n",
        "    def forward(self, X):\n",
        "      ##### START OF YOUR CODE #####\n",
        "      batch_size = X.size(0)\n",
        "      image_size = X.size(2)\n",
        "      \n",
        "      X = self.conv1(X)\n",
        "      X = self.bn(X)\n",
        "      X = self.act(X)\n",
        "      X = self.max_pool(X)\n",
        "      X = self.conv2_x(X)\n",
        "      X = self.conv3_x(X)\n",
        "      X = self.conv4_x(X)\n",
        "      X = self.conv5_x(X) \n",
        "      X = self.avg_pool(X) \n",
        "      X = self.drop_out(X)\n",
        "      X = self.flatten(X)\n",
        "      X = self.fully_connected(X)\n",
        "      X = self.sigmoid(X)\n",
        "      ##### END OF YOUR CODE #####\n",
        "      return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvviIPvsSj6y"
      },
      "outputs": [],
      "source": [
        " from torchsummary import summary\n",
        "\n",
        " device = torch.device('cuda')\n",
        " model = ResNet18(\"relu\", .5, 128).to(device)\n",
        " summary(model, (1, 128, 128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmljrzaQfZ2s"
      },
      "source": [
        "# Assignment 3\n",
        "<p>In this assignment you will\n",
        "\n",
        "* write helper functions\n",
        "* train the model\n",
        "* hyperparameter search using W&B\n",
        "\n",
        "Read the comments carefully and insert your code where you see: <br><br><b>##### START OF YOUR CODE #####</b><br><br><b>##### END OF YOUR CODE #####</b><br><br>or for the inline codes you will see<br><br><b>##### INSERT YOUR CODE HERE #####</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUnVIVdtY3x0"
      },
      "source": [
        "#### I. AverageMeter\n",
        "First we will write a helper function. AverageMeter is to calculate the mean of the running loss and accuracy. \n",
        "\n",
        "*   It will have 2 functions which are reset and update.\n",
        "*   reset will be called on initialization and set the attributes to 0. \n",
        "*   update takes 2 arguments for the value and the size. It will add the value to the sum and the size to the count. Attribute \"avg\" (use this name) will also be updated as sum/count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A38OLNLbYwfa"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "      ##### START OF YOUR CODE #####\n",
        "      self.sum = []\n",
        "      self.size = []\n",
        "      self.avg = 0\n",
        "      self.count = 0\n",
        "\n",
        "      self.reset()\n",
        "\n",
        "    def reset(self): \n",
        "      self.sum = []\n",
        "      self.size = []\n",
        "      self.avg = 0\n",
        "      self.count = 0\n",
        "\n",
        "    def update(self, value, size):\n",
        "   \n",
        "      self.sum.append(value*size)   \n",
        "      self.size.append(size)\n",
        "\n",
        "      sum1 = 0  \n",
        "      size1 = 0    \n",
        "      for i in range(0, len(self.sum)):\n",
        "        sum1 += self.sum[i]\n",
        "        size1 += self.size[i]\n",
        "        self.count = size1\n",
        "        self.avg = sum1/self.count\n",
        "\n",
        "\n",
        "      ##### END OF YOUR CODE #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCFUuYR7agXr",
        "outputId": "dae1ad8b-bd00-4ffb-ee3f-01d7873f52b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75.0 10\n"
          ]
        }
      ],
      "source": [
        "avg_meter = AverageMeter()\n",
        "avg_meter.update(100, 5)\n",
        "avg_meter.update(50, 5)\n",
        "\n",
        "print(avg_meter.avg, avg_meter.count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk3vHad6dhhf"
      },
      "source": [
        "#### II. Train Loop\n",
        "Now we will write the training and validation loops. Detailed instructions are given within the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NqmEhWg6befh"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "def training(train_loader, model, criterion, optimizer):\n",
        "  # Let's start by initializing our AverageMeters.\n",
        "  avg_meters = {'loss': AverageMeter(),\n",
        "                'acc': AverageMeter()}\n",
        "\n",
        "  # We will go through the train_loader.\n",
        "  # Zero the gradients.\n",
        "  # Make prediction.\n",
        "  # Calculate the loss and the accuracy using prediction and labels.\n",
        "  # Update the average meters.\n",
        "  # Compute gradients and adjust learning weights.\n",
        "\n",
        "  ##### START OF YOUR CODE #####\n",
        "    #load data\n",
        "  for data in train_loader:\n",
        "      \n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    #zero gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #prediction\n",
        "    output = model(inputs).to(device)\n",
        "\n",
        "    #loss and accuracy\n",
        "    loss = criterion(output.float(), labels.float())\n",
        "    acc = float((output.round() == labels).float().mean())\n",
        "\n",
        "    avg_meters['loss'].update(loss, len(data))\n",
        "    avg_meters['acc'].update(acc, len(data))\n",
        "\n",
        "    #gradients and learning weights\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  ##### END OF YOUR CODE #####\n",
        "\n",
        "  return dict([('loss', avg_meters['loss'].avg),\n",
        "                ('acc', avg_meters['acc'].avg)])\n",
        "\n",
        "def validation(val_loader, model, criterion):\n",
        "  avg_meters = {'loss': AverageMeter(),\n",
        "                'acc': AverageMeter()}\n",
        "\n",
        "  # Validation is almost the same but don't forget to turn the eval mode of the model and with torch no_grad.\n",
        "  # You don't need to compute gradients or adjust learning weights for evaluation.\n",
        "\n",
        "  ##### START OF YOUR CODE #####\n",
        "  model.eval()\n",
        " \n",
        "  with torch.no_grad():\n",
        "     #load data\n",
        "    for data in val_loader:\n",
        "      \n",
        "      inputs,labels = data\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      #prediction\n",
        "      output = model(inputs).to(device)\n",
        "\n",
        "      #loss and accuracy\n",
        "      loss = criterion(output.float(), labels.float())\n",
        "      acc = float((output.round() == labels).float().mean())\n",
        "\n",
        "      avg_meters['loss'].update(loss, len(data))\n",
        "      avg_meters['acc'].update(acc, len(data))\n",
        "\n",
        "  ##### END OF YOUR CODE #####\n",
        "\n",
        "  return dict([('loss', avg_meters['loss'].avg),\n",
        "              ('acc', avg_meters['acc'].avg)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test function\n",
        "\n",
        "def testing(test_loader, model, criterion):\n",
        "  avg_meters = {'loss': AverageMeter(),\n",
        "                'acc': AverageMeter()}\n",
        "  \n",
        "  model.eval()\n",
        " \n",
        "  with torch.no_grad():\n",
        "     #load data\n",
        "    for data in test_loader:\n",
        "      \n",
        "      inputs,labels = data\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      #prediction\n",
        "      output = model(inputs).to(device)\n",
        "\n",
        "      #accuracy\n",
        "      loss = criterion(output.float(), labels.float())\n",
        "      acc = float((output.round() == labels).float().mean())\n",
        "      \n",
        "      avg_meters['loss'].update(loss, len(data))\n",
        "      avg_meters['acc'].update(acc, len(data))\n",
        "\n",
        "  ##### END OF YOUR CODE #####\n",
        "\n",
        "  return dict([('loss', avg_meters['loss'].avg),\n",
        "               ('acc', avg_meters['acc'].avg)])\n"
      ],
      "metadata": {
        "id": "Q4UNyqKRsGju"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1ZChNtVfyRw"
      },
      "source": [
        "We will use Weights & Biases for hyperparameter search. This will only be an introduction and we highly recommend you to read the <a href=\"https://docs.wandb.ai/?_gl=1*1xon9b*_ga*NDg5OTYzNTM3LjE2NzUwNjYzNjk.*_ga_JH1SJHJQXJ*MTY3Njc0MDEyNi4xMi4xLjE2NzY3NDAxMjguNTguMC4w\">documentation</a> for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4uhE_s3cRvo"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"/content/gdrive/MyDrive/CMPE_runs/\")"
      ],
      "metadata": {
        "id": "Ei4rcV7L95Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rWk9ArIm_hoQ"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "import wandb\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def main():\n",
        "  # Set the initial configuration\n",
        "  initial_config = {\n",
        "      \"data_dir\": \"/tmp/curated_data/data/\",\n",
        "      \"image_size\": 128,\n",
        "      \"train_batch_size\": 64,\n",
        "      \"val_batch_size\": 32,\n",
        "      \"test_batch_size\": 1,\n",
        "      \"activation\": \"relu\",\n",
        "      \"drop_rate\": .2,\n",
        "      \"optimizer\": \"Adam\",\n",
        "      \"learning_rate\": 1e-3,\n",
        "      \"l2_reg\": 1e-4, # Weight decay\n",
        "      \"nb_epoch\": 50,\n",
        "      \"early_stopping\": 15, # trigger value for early stopping\n",
        "      \n",
        "  }\n",
        "\n",
        "  # Using this configuration dictionary:\n",
        "  # initialize wandb\n",
        "  # Create a run directory in your drive (\"/content/drive/MyDrive/CMPE_runs/\" + the current run name that you'll get from wandb)\n",
        "  # Create the model\n",
        "  # Create dataloader dictionary with \"train\", \"val\", \"test\" keys\n",
        "  # Define binary cross entropy loss\n",
        "  # Define optimizer with weight decay\n",
        "  # Set lr scheduler to ReduceLROnPlateau:\n",
        "    # It will decrease the lr by .1 if the val_loss did not decrease > .01. The minimum lr value can be 1e-9.\n",
        "  # Print train and val results and log them to wandb at the end of each epoch\n",
        "  # Save best model weights to your run directory when the val accuracy is at least .01 better than the best val accuracy.\n",
        "  # Set early stopping with the trigger in config[\"early_stopping\"], monitoring val accuracy. config[\"early_stopping\"] = -1 means no early stopping.\n",
        "  # Print when a new model is saved or early stopping trigger is reached.\n",
        "  # After the final epoch (or early stopping), load the best model weights and log the test results to wandb*\n",
        "\n",
        "  ##### START OF YOUR CODE #####\n",
        "  #init wandb\n",
        "  wandb.init(\n",
        "      config = initial_config,\n",
        "      project = \"cmpe58p_assignment\",\n",
        "      reinit = True  \n",
        "  )\n",
        "  \n",
        "  #directories\n",
        "  run_name = wandb.run.name\n",
        "  run_path = \"/content/gdrive/MyDrive/CMPE_runs/\" + run_name + \"/\"\n",
        "  save_path = run_path  + \"models.pth\"\n",
        "  os.mkdir(run_path)\n",
        "  \n",
        "\n",
        "  #model\n",
        "  model = ResNet18(initial_config['activation'], initial_config['drop_rate'], initial_config['image_size']).to(device)\n",
        "\n",
        "  #dataloader dictionary\n",
        "  image_size = initial_config['image_size']\n",
        "  data_dir = initial_config['data_dir']\n",
        "\n",
        "  data_dict = {x: CustomDataset(image_size, data_dir, x) for x in [\"train\", \"val\", \"test\"]}\n",
        "  \n",
        "  dataloader = {x: DataLoader(data_dict[x], initial_config[f'{x}''_batch_size']) for x in [\"train\", \"val\", \"test\"]}\n",
        "\n",
        "  #some important stuff\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=initial_config['learning_rate'], weight_decay = initial_config['l2_reg'])\n",
        "  lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, threshold = 0.01,  min_lr = 1e-9)\n",
        "  best_val_acc = 0\n",
        "  patience = initial_config['early_stopping']\n",
        "  trigger_times = 0\n",
        "  #training and validation\n",
        "  for epoch in range(1, initial_config['nb_epoch']+1):\n",
        "   \n",
        "\n",
        "    wandb.watch(model, criterion, log = \"all\")\n",
        "\n",
        "    train = training(dataloader['train'], model, criterion, optimizer)\n",
        "    \n",
        "    val = validation(dataloader['val'], model, criterion)\n",
        "\n",
        "    lr_scheduler.step(val['loss'])\n",
        "\n",
        "    \n",
        "    if val['acc'] > best_val_acc:\n",
        "      trigger_times = 0\n",
        "\n",
        "    if val['acc'] - best_val_acc <= 0:\n",
        "      trigger_times += 1\n",
        "      if trigger_times == patience:\n",
        "        wandb.log({'epoch' : epoch, 'train_loss' : train['loss'], 'train_acc' : train['acc'], 'val_loss' : val['loss'], 'val_acc' : val['acc']})\n",
        "        print(f\"epoch : {epoch}'  train_loss : {train['loss']}  train_acc : {train['acc']}  val_loss : {val['loss']}  val_acc : {val['acc']}\")\n",
        "        print(\"Early Stopping!\")\n",
        "        break\n",
        "\n",
        "    wandb.log({'epoch' : epoch, 'train_loss' : train['loss'], 'train_acc' : train['acc'], 'val_loss' : val['loss'], 'val_acc' : val['acc']})\n",
        "    print(f\"epoch : {epoch}'  train_loss : {train['loss']}  train_acc : {train['acc']}  val_loss : {val['loss']}  val_acc : {val['acc']}\")\n",
        "\n",
        "    if val['acc'] - best_val_acc >= 0.01:\n",
        "      torch.save(model.state_dict(), save_path)\n",
        "      best_val_acc = val['acc']\n",
        "      print(\"The model is saved\")\n",
        "    \n",
        "  \n",
        "  #testing\n",
        "  model.load_state_dict(torch.load(save_path))\n",
        "  model.eval()\n",
        "  tester = testing(dataloader['test'], model, criterion)\n",
        "  wandb.log({'Test Result ' : tester['acc']})\n",
        "  print(f\"Test Result : {tester['acc']}\" )\n",
        "  \n",
        "      \n",
        "  ##### END OF YOUR CODE #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwcd18kWfcTT"
      },
      "outputs": [],
      "source": [
        "# Define the parameters that we will fine tune with, which are:\n",
        "  # Activation function\n",
        "  # Optimizer\n",
        "  # Drop rate: should be chosen randomly from a uniform distribution between [0., 0.9]\n",
        "  # Weight decay: should be chosen randomly from a uniform distribution between [0., 0.1]\n",
        "  # Learning rate: should be chosen randomly from a uniform distribution between [0.0001, 0.1]\n",
        "  \n",
        "parameter_dict = {\n",
        "    ##### START OF YOUR CODE #####\n",
        "    'activation_function' : {'values' : ['relu', 'leaky_relu', 'gelu']},\n",
        "    'optimizer' : {'values' : ['Adam', 'SGD', 'RMSprop']},\n",
        "    'drop_rate' : {'min' : .0, 'max' : 0.9},\n",
        "    'weight_decay' : {'min' : .0, 'max' : 0.1},\n",
        "    'lr_rate' : {'min' : 0.0001, 'max' : 0.1}\n",
        "    ##### END OF YOUR CODE #####\n",
        "}\n",
        "\n",
        "# Define a sweep configuration tells wandb that it will randomly choose from parameter dict and the purpose is to maximize val_accuracy\n",
        "sweep_config = {\n",
        "    ##### START OF YOUR CODE #####\n",
        "    'method' : 'random',\n",
        "    'name' : 'sweep',\n",
        "    'metric' : {'goal' : 'maximize', 'name' : 'val_acc'},\n",
        "    'parameters' : {}\n",
        "    ##### END OF YOUR CODE #####\n",
        "}\n",
        "sweep_config['parameters'] = parameter_dict\n",
        "\n",
        "# Start the sweep\n",
        "sweep_id = wandb.sweep(\n",
        "    sweep = sweep_config,\n",
        "    project = 'cmpe58p_assignment'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb5ydeudBDSK"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, function=main)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}